{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import  Variable \n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from AttentionTransformer.TrainClassificationTransformer import * \n",
    "from AttentionTransformer.ClassificationDataset import *\n",
    "from AttentionTransformer.utilities import count_model_parameters\n",
    "import pickle \n",
    "from tqdm import tqdm_notebook, tqdm, trange, tnrange \n",
    "import torch.optim as optim\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import logging\n",
    "import sys\n",
    "sys.path.append('../scripts/')\n",
    "\n",
    "from ClassificationDatasetFromDict import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 3007\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "def load_pickle(filepath):\n",
    "    with open(filepath, 'rb') as fp:\n",
    "        return pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertWordPieceTokenizer('../data/bert-word-piece-custom-wikitext-vocab-10k-vocab.txt', lowercase = True, strip_accents = True)\n",
    "\n",
    "\n",
    "data = load_pickle('../data/tokenized_questions_classes_subclasses_dict.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttentionTransformer.Encoder import * \n",
    "from AttentionTransformer.Decoder import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneEncoderTwoDecoderTransformer(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, vocab_size, pad_id, CLS_label_id, emb_dim = 512, dim_model = 512, dim_inner = 2048,\n",
    "        layers = 6, heads = 8, dim_key = 64, dim_value = 64, dropout = 0.1, num_pos = 200\n",
    "    ):\n",
    "\n",
    "        super(OneEncoderTwoDecoderTransformer, self).__init__()\n",
    "\n",
    "        self.pad_id = pad_id \n",
    "        self.encoder = Encoder(\n",
    "            vocab_size, emb_dim, layers, heads, dim_key, dim_value, dim_model, dim_inner, pad_id, dropout = dropout, num_pos = num_pos\n",
    "        )\n",
    "\n",
    "        self.decoder1 = Decoder(\n",
    "            vocab_size, emb_dim, layers, heads, dim_key, dim_value, dim_model, dim_inner, pad_id, dropout = dropout, num_pos = num_pos\n",
    "        )\n",
    "\n",
    "        self.decoder2 = Decoder(\n",
    "            vocab_size, emb_dim, layers, heads, dim_key, dim_value, dim_model, dim_inner, pad_id, dropout = dropout, num_pos = num_pos\n",
    "        )\n",
    "\n",
    "        self.decoder1heads = nn.Linear(dim_model, 6)\n",
    "\n",
    "        self.decoder2heads = nn.Linear(dim_model, 47)\n",
    "\n",
    "        for parameter in self.parameters():\n",
    "\n",
    "            if parameter.dim() > 1:\n",
    "\n",
    "                nn.init.xavier_uniform_(parameter)\n",
    "\n",
    "        assert dim_model == emb_dim, f'Dimensions of all the module objects must be same'\n",
    "\n",
    "        self.cls_label_id = CLS_label_id\n",
    "\n",
    "    def get_pad_mask(self, sequence, pad_id):\n",
    "\n",
    "        return (sequence != pad_id).unsqueeze(-2)\n",
    "\n",
    "    def get_subsequent_mask(self, sequence):\n",
    "\n",
    "        batch_size, seq_length = sequence.size() \n",
    "\n",
    "        subsequent_mask = (\n",
    "            1 - torch.triu(\n",
    "                torch.ones((1, seq_length, seq_length), device=sequence.device), diagonal = 1\n",
    "            )\n",
    "        ).bool()\n",
    "\n",
    "        return subsequent_mask\n",
    "\n",
    "    def make_target_seq(self, batch_size):\n",
    "\n",
    "        trg_tnsr = torch.zeros((batch_size, 1))\n",
    "        trg_tnsr[trg_tnsr == 0] = self.cls_label_id\n",
    "        return trg_tnsr.float()\n",
    "\n",
    "    def get_decoder2_target(self, labels):\n",
    "\n",
    "        tnsr = labels.float()\n",
    "        return tnsr.unsqueeze(1)\n",
    "\n",
    "    def forward(self, source_seq, classlabels):\n",
    "\n",
    "        b, l = source_seq.size()\n",
    "        targetdec1 = self.make_target_seq(b).to(source_seq.device)\n",
    "        source_mask = self.get_pad_mask(source_seq, self.pad_id)\n",
    "        targetdec1_mask = self.get_pad_mask(targetdec1, self.pad_id) & self.get_subsequent_mask(targetdec1)\n",
    "\n",
    "        targetdec2 = self.get_decoder2_target(classlabels)\n",
    "        targetdec2_mask = self.get_pad_mask(targetdec2, self.pad_id) & self.get_subsequent_mask(targetdec2)\n",
    "        \n",
    "\n",
    "        encoder_output = self.encoder(source_seq, source_mask)\n",
    "        decoder_output_1 = self.decoder1(\n",
    "            targetdec1, targetdec1_mask, encoder_output, source_mask\n",
    "        )\n",
    "        decoder_output_2 = self.decoder2(\n",
    "            targetdec2, targetdec2_mask, encoder_output, source_mask\n",
    "        )\n",
    "\n",
    "        decoder_output_1 = decoder_output_1.view(decoder_output_1.size(0), -1)\n",
    "        decoder_output_2 = decoder_output_2.view(decoder_output_2.size(0), -1)\n",
    "\n",
    "        classheads1 = self.decoder1heads(decoder_output_1)\n",
    "        classheads2 = self.decoder2heads(decoder_output_2)\n",
    "\n",
    "        return classheads1, classheads2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ClassificationDatasetDict(data, 100)\n",
    "dataloader = DataLoader(dataset, batch_size = 4, shuffle = True, num_workers=3, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OneEncoderTwoDecoderTransformer(\n",
    "    vocab_size = tokenizer.get_vocab_size(), \n",
    "    pad_id = 2,\n",
    "    CLS_label_id = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "84.691509"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "count_model_parameters(model) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[ 0.0669, -1.5990,  0.7057,  0.4327,  1.2237,  2.1680],\n",
       "         [-0.3351, -0.5885,  2.0430,  3.5518, -0.6105,  0.4860],\n",
       "         [-1.4577, -0.8539,  2.8529,  0.5232,  0.8898, -1.9092],\n",
       "         [-1.1834, -1.2539,  2.2092,  1.5802, -1.4747,  2.8295]],\n",
       "        grad_fn=<AddmmBackward>),\n",
       " tensor([[ 0.0231, -0.9397, -1.3733,  2.1185,  0.1620,  0.3008, -1.9727,  1.0801,\n",
       "          -1.2780,  0.0189,  0.2631,  0.6690, -0.4242, -0.0549,  4.2453,  0.3284,\n",
       "           1.2134, -1.4837,  0.9786, -2.7903, -1.1144,  0.6317, -2.3783, -0.3723,\n",
       "           1.2815,  0.9917,  1.5092,  2.2412, -0.3041, -1.2079,  1.2051,  0.8503,\n",
       "          -1.4927,  0.9340,  1.8753,  3.8063, -1.1820,  0.8290, -2.3491,  2.4052,\n",
       "          -2.0367,  0.1661,  0.7350,  1.4218,  0.5079, -3.6047, -1.2469],\n",
       "         [ 2.2929, -1.6826, -2.1070,  0.1964,  1.6595,  0.0487, -2.3043, -2.3063,\n",
       "          -1.1869, -1.4780,  0.2604, -0.3642, -0.7189,  1.3293,  4.5082,  1.4669,\n",
       "           0.9167, -1.5544,  1.1655, -0.7214, -0.6193,  1.6200, -1.8363, -1.2395,\n",
       "          -0.5588,  0.0112,  1.5795,  2.3488, -0.4565, -0.2107,  1.6604,  0.1640,\n",
       "          -0.0294, -0.0643, -1.0216,  2.3868,  2.1730,  1.1965, -0.6235,  1.7862,\n",
       "          -1.1892, -1.2581,  1.3192,  1.3941, -0.3970, -2.6505, -1.1288],\n",
       "         [ 1.5972,  1.6897, -0.7385, -1.3267,  1.0559, -0.1400, -1.5179, -1.9370,\n",
       "          -0.2712, -1.0465,  0.7733,  1.7623, -1.3255, -0.1150,  2.5156, -0.5806,\n",
       "           2.5031, -1.1573,  1.1969, -1.1191, -0.5838, -1.6926, -2.0427, -1.7693,\n",
       "           1.5413,  1.8514,  0.6432,  2.5173,  1.9117, -2.1518,  0.9888,  2.3239,\n",
       "           0.6347,  0.9519,  1.0487,  0.0926,  0.8825, -0.5589, -2.4771,  2.1242,\n",
       "           0.6718, -1.4317,  2.3423,  1.5067, -0.2628, -4.8708, -0.7213],\n",
       "         [ 1.3327,  0.8381,  0.9281, -1.6982, -1.2858, -1.1108, -1.0243,  1.4718,\n",
       "          -2.2629, -2.5409,  0.6859,  1.2290, -2.3397, -0.0332,  1.6740,  1.8300,\n",
       "           0.2947, -2.4285, -0.4696, -1.2003, -0.4003,  1.4698, -2.2321, -1.0324,\n",
       "           1.1355, -0.2345, -0.4559,  1.3573, -0.8247,  2.3301,  1.4943,  2.2234,\n",
       "          -0.3691,  0.8447, -0.6347, -0.7842,  2.6901, -0.5268,  0.4317,  2.2354,\n",
       "           0.8588, -1.3237, -0.5471, -0.2584,  0.3787, -1.4912, -2.7330]],\n",
       "        grad_fn=<AddmmBackward>))"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# model(d['source_seq'], d['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}